---
title: "Homework 5 Applied Exercises"
author: "Suprotik Debnath"
date: "3/4/2019"
output:
  pdf_document: default
  word_document: default
---

Question 9)

Part A)
```{r}
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
library(base)
data(College)
set.seed(11)
training = sample(1:dim(College)[1],dim(College)[1]/2)

testing <-- training

college.train = College[training,]
college.test = College[testing,]
```


Part B)
```{r}
fit.lm = lm(Apps~.,data = college.train)

pred.lm = predict(fit.lm,college.test)

mean((pred.lm - college.test$Apps)^2)
```
The least square value is approximately 1538442. 


Part C)
```{r}
train.mat = model.matrix(Apps~.,data = college.train)
test.mat = model.matrix(Apps~.,data = college.test)
grid = 10^seq(4,-2,length = 100)
fit.ridge = glmnet(train.mat,college.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge = cv.glmnet(train.mat,college.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)

bestlam.ridge = cv.ridge$lambda.min
bestlam.ridge

```
The value we get from this test is approximately 18.73817. 

```{r}
pred.ridge = predict(fit.ridge,s=bestlam.ridge,newx = test.mat)
```


Part D)
```{r}
fit.lasso = glmnet(train.mat,college.train$Apps, alpha=1, lambda=grid, thresh = 1e-12)
cv.lasso = cv.glmnet(train.mat,college.train$Apps, alpha=1, lambda=grid, thresh = 1e-12)

bestlam.lasso = cv.lasso$lambda.min
bestlam.lasso

pred.lasso = predict(fit.lasso,s=bestlam.lasso,newx=test.mat)
mean((pred.lasso-college.test$Apps)^2)

predict(fit.lasso,s=bestlam.lasso,type="coefficients")
```
The two outputs that we obtain are 21.54435 and 1635280. Additionally, we can show the predictions that were made in output generated by the matrix. 


Part E) 
```{r}
fit.pcr = pcr(Apps~.,data = college.train, scale=TRUE, validation="CV")
validationplot(fit.pcr,val.type="MSEP")
pred.pcr <- predict(fit.pcr,college.test,ncomp=10)
mean((pred.pcr-college.test$Apps)^2)
```
This is the plot we get with the PCR and the output value is equal to 3014496.


Part F)
```{r}
fit.pls = plsr(Apps~.,data=college.train,scale=TRUE,validation="CV")
validationplot(fit.pls,val.type="MSEP")
pred.pls = predict(fit.pls,college.test,ncomp=10)
mean((pred.pls-college.test$Apps)^2)

```
This is the output we get from the plsr() function. The output value that we get is 1508987.


Part G)
```{r}
test.avg = mean(college.test$Apps)
lm.r2= 1 - mean((pred.lm - college.test$Apps)^2)/mean((test.avg-college.test$Apps)^2)
ridge.r2 = 1 - mean((pred.ridge - college.test$Apps)^2)/mean((test.avg-college.test$Apps)^2)
lasso.r2 = 1 - mean((pred.lasso - college.test$Apps)^2)/mean((test.avg-college.test$Apps)^2)
pcr.r2 = 1 - mean((pred.pcr - college.test$Apps)^2)/mean((test.avg-college.test$Apps)^2)
pls.r2 = 1 - mean((pred.pls - college.test$Apps)^2)/mean((test.avg-college.test$Apps)^2)

lm.r2
ridge.r2
lasso.r2
pcr.r2
pls.r2
```
The output shows us that all of these models should be able to predict the college applications rather effectively. However, PCR is somewhat questionable as it has a lower output than the rest of them (0.81 compared to a general average of 0.9).




Question 10)

Part A)
```{r}
set.seed(1)

x=matrix(rnorm(1000*20),1000,20)
b = rnorm(20)
b[3] = 0
b[4]= 0
b[9]= 0
b[19]= 0
b[10]= 0

eps= rnorm(1000)
y = x%*%b + eps

```


Part B)
```{r}
train = sample(seq(1000),100,replace = FALSE)
test <-- train
x.train = x[train,]
x.test = x[test,]
y.train = y[train]
y.test = y[test]
```


Part C)
```{r}
data.train =data.frame(y=y.train,x=x.train)
regfit.full = regsubsets(y~.,data=data.train,nvmax = 20)
train.mat = model.matrix(y~.,data=data.train,nvmax = 20)

val.errors = rep(NA,20)
for(i in 1:20)
{
    coefi=coef(regfit.full,id=i)
    pred = train.mat[,names(coefi)]%*%coefi
    val.errors[i]=mean((pred-y.train)^2)
    
}
plot(val.errors,xlab="Number of predictors", ylab = "Training MSE", pch=19, type = "b")
```
  
  
Part D)
```{r}
data.test=data.frame(y=y.test,x=x.test)

test.mat=model.matrix(y~.,data = data.test, nvmax=20)
val.errors=rep(NA,20)

for(i in 1:20)
{
    coefi=coef(regfit.full,id=i)
    pred = test.mat[,names(coefi)]%*%coefi
    val.errors[i]=mean((pred-y.test)^2)
}
plot(val.errors,xlab="Number of predictors", ylab = "Training MSE", pch=19, type = "b")
```
  
  
Part E)
```{r}
which.min(val.errors)
```
The minimum sum of squares among all the models observed is approximately 14.


Part F)
```{r}
coef(regfit.full,which.min(val.errors))
```
The model for the sum of squares is shown in this output. The best model caught all zeroed out coefficients.


Part G)
```{r}
val.errors = rep(NA,20)
x_cols = colnames(x,do.NULL = FALSE, prefix = 'x.')
for(i in 1:20)
{ 
    coefi = coef(regfit.full, id=i)
    val.errors[i]= sqrt(sum((b[x_cols%in%names(coefi)]-coefi[names(coefi)%in%x_cols])^2) + sum(b[!(x_cols%in%names(coefi))])^2)
}

plot(val.errors,xlab ="Number of coefficients", ylab = "Error between estimated and true coefficients", pch=19, type = "b")
```
We can see that the model with 3 variables generally minimizes the error between the estimated and true coefficients. However, the test error is minimized by the model with 14 variables. Therefore, we can argue that a better fit of true coefficients doesn't necessarily mean a lower test MSE in a given model.



Question 6 (Exercise 9 Repeated)) 
```{r}
library(readr)
setwd("~/University of Pittsburgh Classes/Junior Second Semester Classes/Statistical Learning and Data Science/OnlineNewsPopularity")
OnlineNewsPopularity <- read.csv("~/University of Pittsburgh Classes/Junior Second Semester Classes/Statistical Learning and Data Science/OnlineNewsPopularity/OnlineNewsPopularity.csv")
training = sample(1:dim(OnlineNewsPopularity)[1],dim(OnlineNewsPopularity)[1]/2)

testing <-- training

newspop.train = OnlineNewsPopularity[training,]
newspop.test = OnlineNewsPopularity[testing,]

fit.lm = lm(shares~.,data = newspop.train)

pred.lm = predict(fit.lm,newspop.test)

mean((pred.lm - newspop.test$shares)^2)
```

```{r}
train.matrix = model.matrix(shares~.,data = newspop.train)
test.matrix = model.matrix(shares~.,data = newspop.test)
grid = 10^seq(4,-2,length = 100)

fit.ridge2 = glmnet(train.matrix,newspop.train$shares, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge2 = cv.glmnet(train.matrix,newspop.train$shares, alpha = 0, lambda = grid, thresh = 1e-12)

bestlam.ridge2 = cv.ridge2$lambda.min
bestlam.ridge2

pred.ridge2 = predict(fit.ridge2,s=bestlam.ridge2,newx = test.matrix)
```
```{r}
fit.lasso2 = glmnet(train.matrix,newspop.train$shares, alpha=1, lambda=grid, thresh = 1e-12)
cv.lasso2 = cv.glmnet(train.matrix,newspop.train$shares, alpha=1, lambda=grid, thresh = 1e-12)

bestlam.lasso2 = cv.lasso2$lambda.min
bestlam.lasso2

pred.lasso2 = predict(fit.lasso2,s=bestlam.lasso2,newx=test.matrix)
mean((pred.lasso2-newspop.test$shares)^2)

predict(fit.lasso2,s=bestlam.lasso2,type="coefficients")
```
```{r}
fit.pcr2 = pcr(shares~.,data = newspop.train, scale=TRUE, validation="CV")
validationplot(fit.pcr2,val.type="MSEP")
pred.pcr2 <- predict(fit.pcr2,newspop.test,ncomp=10)
mean((pred.pcr2-newspop.test$shares)^2)
```

```{r}
fit.pls2 = plsr(shares~.,data=newspop.train,scale=TRUE,validation="CV")
validationplot(fit.pls2,val.type="MSEP")
pred.pls2 = predict(fit.pls2,newspop.test,ncomp=10)
mean((pred.pls2-newspop.test$shares)^2)
```
```{r}
test.avg2 = mean(newspop.test$shares)
lm.r3= 1 - mean((pred.lm - newspop.test$shares)^2)/mean((test.avg-newspop.test$shares)^2)
ridge.r3 = 1 - mean((pred.ridge - newspop.test$shares)^2)/mean((test.avg-newspop.test$shares)^2)
lasso.r3 = 1 - mean((pred.lasso - newspop.test$shares)^2)/mean((test.avg-newspop.test$shares)^2)
pcr.r3 = 1 - mean((pred.pcr - newspop.test$shares)^2)/mean((test.avg-newspop.test$shares)^2)
pls.r3 = 1 - mean((pred.pls - newspop.test$shares)^2)/mean((test.avg-newspop.test$shares)^2)

lm.r3
ridge.r3
lasso.r3
pcr.r3
pls.r3
```

